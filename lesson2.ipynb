{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Искусственные нейронные сети (урок 2).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"BRWep_TI-JGY","colab_type":"text"},"cell_type":"markdown","source":["#Введение"]},{"metadata":{"id":"hE8T2n_OyG35","colab_type":"text"},"cell_type":"markdown","source":["Сегодня мы поговорим о задачах обработки естественного языка.\n","<br> Обработкой естественного языка (NLP) называется активно развивающаяся научная дисциплина, занимающаяся поиском смысла и обучением на основании текстовых данных.\n","<br> Решение задач nlp опять же дает вам целую кучу возможностей. Вы можете классифицировать текст: например, заставить алгоритм отвечать на определенные вопросы; производить анализ тональности текста (собственно, это то, чем вы занимались в домашнем задании): например, для рейтинговой системы фильмов или для предсказания цен на крипту."]},{"metadata":{"id":"PAuoJBjoyHZJ","colab_type":"text"},"cell_type":"markdown","source":["# Теория\n"]},{"metadata":{"id":"-75NKQgCNOSo","colab_type":"text"},"cell_type":"markdown","source":["## Векторные представления слов (a.k.a. word embeddings)"]},{"metadata":{"id":"I6TaOsIayHbn","colab_type":"text"},"cell_type":"markdown","source":["Зачем это вообще надо. Текст сам по себе очень беспорядочный (то есть он может состоять из разного количества слов, причем слова все разной длины и тд). А алгоритмы типа нейронных сетей не предназначены для того, чтобы информация поступала в разных видах на ее входы. В связи с этим возникает необходимость создавать модели слов, которые можно использовать в алгоритмах машинного обучения. Такие модели представляют из себя цифры, а точнее последовательности цифр (a.k.a векторы)\n","<br>На сегодняшний день существует целая куча различных алгоритмов для перевода текста в более преемлимый вид. Среди них bag-of-words, tf-idf, word2vec, fasttext  и так далее. Важно отметить, что представление слов - это не просто присуждение номеров словам, а как бы отображение в цифрах сущности слова или целого предложения (то есть представление слова дает нам информацию о его лексическом и грамматическом значении). \n","<br> Обсудим некоторые из вышеперечисленных алгоритмов\n"]},{"metadata":{"id":"dBwc8zNvyHgL","colab_type":"text"},"cell_type":"markdown","source":["### Bag of Words (BoW)"]},{"metadata":{"id":"AaKWRnMpVmZK","colab_type":"text"},"cell_type":"markdown","source":["Bage-of-words - это самый первый алгоритм, с которого все обычно начинают свой путь к познанию word embeddings. И мы тоже не будем нарушать традиции. \n","<br> BoW - это один из способов извлечения признаков из текста (feature extraction). Смысл достаточно простой. Мы берем все знакомые нам слова и заводим счетчик **для каждого** из них. Берем предложение, смотрим на него, обновляем счетчики. В итоге получаем векторное представление для всего предложения. Обратите внимание, что в словарь можно включать не только слова, но и сочетания слов (или n-граммы) и считать уже количество сочетаний."]},{"metadata":{"id":"Lf1WJtXUa56V","colab_type":"text"},"cell_type":"markdown","source":["Допустим наш словарь состоит из слов: [егор, пошел, гулять, помидор, в, магазин]. И есть три предложения: 1. егор пошел гулять, 2. егор помидор, 3. егор пошел в магазин.\n","В этом случае представление предложений будет следующим:\n","1. [1, 1, 1, 0, 0, 0]\n","2. [1, 0, 0, 1, 0, 0]\n","3. [1, 1, 0, 0, 1, 1]"]},{"metadata":{"id":"GNL8d-84_Mfk","colab_type":"text"},"cell_type":"markdown","source":["### tf-idf"]},{"metadata":{"id":"faoNCvFGa086","colab_type":"text"},"cell_type":"markdown","source":["Но в таком случае возникает проблема: если у нас текст большой и в нем есть много повторяющихся слов, которые не несут особого смысла (типа всякие союзы, частицы, слова-паразиты и тд).\n","\n","Специально для этого придумали другой способ -- tf-idf (Term Frequency – Inverse Document Frequency). tf - это отношение числа вхождений какого-то слова к общему количеству слов в предложении, а idf - это логарифм отношения общего числа предложений к числу предложений, в которых встречается наше слово. Для того, чтобы получить репрезентацию слова нужно tf умножить на idf. Таким образом понижается важность частовстречающихся слов"]},{"metadata":{"id":"W3Lny7ugv2zc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"90f53dd4-4ff0-4339-f774-261a4f9aba05","executionInfo":{"status":"ok","timestamp":1541053966883,"user_tz":-180,"elapsed":622,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["import numpy as np\n","from math import log\n","\n","text_base = [\"егор пошел гулять\",\n","             \"егор помидор\",\n","             \"егор пошел в магазин\"]\n","\n","# составляем список слов\n","# делаем из него сет(множество), чтобы убрать дубли\n","# преобразуем опять в список, чтобы удобнее было работать\n","words = [i for ws in text_base for i in ws.split()]\n","vocab = set(words)\n","vocab = list(vocab)\n","\n","# параметры векторов tf-idf\n","n_docs = len(text_base)\n","vec_len = len(vocab)\n","\n","tf_vecs = np.empty([n_docs, vec_len]) # сюда будем складывать tf\n","vec = np.empty([0, vec_len]) # вспомогательный массив\n","\n","# считаем tf\n","for i in range(n_docs):\n","    n_words = len(text_base[i].split()) # кол-во слов в предложении\n","    tf = np.asarray([]) # временное хранение\n","    for one in vocab:\n","        tf = np.append(tf, text_base[i].count(one)) # подсчитываем кол-во слов в предложении\n","    tf = tf / n_words # делим на кол-во слов каждый элемент в массиве\n","    tf_vecs[i] = np.vstack((vec, tf)) # заполняем массив tf\n","print(\"tf:\")\n","print(tf_vecs)\n","\n","idf = np.asarray([]) # сюда складываем idf\n","\n","# считаем idf\n","for one in vocab:\n","    # считаем, в скольких предложениях есть наше слово\n","    cnt = 0\n","    for text in text_base:\n","        if text.count(one) > 0:\n","            cnt += 1\n","    # считаем\n","    idf = np.append(idf, log(n_docs/cnt))\n","print(\"idf:\")\n","print(idf)\n","\n","# считаем tf-idf\n","for i in range(n_docs):\n","    tf_vecs[i] = tf_vecs[i] * idf\n","    \n","tf_idf = tf_vecs\n","print(\"tf_idf:\")\n","print(tf_idf)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["tf:\n","[[0.         0.33333333 0.         0.33333333 0.33333333 0.        ]\n"," [0.5        0.         0.         0.         0.5        0.        ]\n"," [0.         0.25       0.25       0.         0.25       0.25      ]]\n","idf:\n","[1.09861229 0.40546511 1.09861229 1.09861229 0.         1.09861229]\n","tf_idf:\n","[[0.         0.13515504 0.         0.3662041  0.         0.        ]\n"," [0.54930614 0.         0.         0.         0.         0.        ]\n"," [0.         0.10136628 0.27465307 0.         0.         0.27465307]]\n"],"name":"stdout"}]},{"metadata":{"id":"D9BVj671LU6-","colab_type":"text"},"cell_type":"markdown","source":["### Word2Vec"]},{"metadata":{"id":"8_B4fSh0LU4K","colab_type":"text"},"cell_type":"markdown","source":["Теперь интереснее. BoW и tf-idf могут дать нам лишь относительное (относительно контекста) представление о слове. Но существуют методы, которые находят связи между многими контекстами и формируют абсолютное представление слова (то есть у нас представление слова всегда одинаково в любом предложении). Такими методами являются word2vec (от гугла) и fasttext (от фейсбука). Расскажу только про word2vec, так как fasttext не особо отличается (ну и про него легче искать информацию)."]},{"metadata":{"id":"rVZYCPmHSgGM","colab_type":"text"},"cell_type":"markdown","source":["Простыми словами: word2vec — это инструмент (набор алгоритмов) для расчета векторных представлений слов, реализует две основные архитектуры — Continuous Bag of Words (CBOW) и Skip-gram. На вход подается корпус текста, а на выходе получается набор векторов слов.\n","\n","Если не углубляться во всякие формулы, то идея проста: в одинаковом контексте слова должны иметь схожее значение (то есть они семантически близки друг к другу). Более формально задача стоит так: максимизация косинусной близости между векторами слов (скалярное произведение векторов), которые появляются рядом друг с другом, и минимизация косинусной близости между векторами слов, которые не появляются друг рядом с другом. Рядом друг с другом в данном случае значит в близких контекстах.\n","\n","Пример:\n","\n","Например, слова «анализ» и «исследование» часто встречаются в похожих контекстах, вроде «Ученые провели анализ алгоритмов» или «Ученые провели исследование алгоритмов». Word2vec анализирует эти контексты и делает вывод, что слова «анализ» и «исследование» являются близкими по смыслу. Так как подобные выводы word2vec делает на основании большого количества текста, выводы оказываются вполне адекватными. Скажем, когда я тренировал word2vec на коллекции из 10 000 небольших научных текстов (что, вообще-то, маловато), то для слова «Испания» на готовой модели получил следующий список наиболее близких по смыслу слов — Италия, Австралия, Нидерланды, Португалия, Франция. То есть, всё вполне четко, за исключением, разве что, Австралии.\n","\n","После обучения модели можно проводить весьма интересные эксперименты:\n","* Искать семантически близкие слова:\n","<br>Enter word or sentence (EXIT to break): кофе\n","<br>— коффе 0.734483\n","<br>чая 0.690234\n","<br>чай 0.688656\n","<br>капучино 0.666638\n","\n","* Складывать и вычитать разные слова:\n","<br>\"жизнь\" - \"смерть\" = \"самообразование\" (Word2Vec плохого не посоветует;))\n","<br>\"король\" - \"мужчина\" + \"женщина\" = \"королева\"\n","<br> и т.д.\n","\n","* Оценка важности слов в запросе\n","<br>Принцип оценки прост. Надо определить, к какому кластеру тяготеет запрос в целом, а потом выбрать слова, максимально удалённые от центра этого кластера. Такие слова и будут главными, а остальные — уточняющими.\n","<br>1. Enter word or sentence (EXIT to break): **владимир путин**\n","<br>Importance владимир = 0.28982\n","<br>Importance путин = 1\n","<br>2. Enter word or sentence (EXIT to break): **никита путин**\n","<br>Importance никита = 0.793377\n","<br>Importance путин = 0.529835\n","* И многое другое...;)\n","\n","На этом с векторными представлениями слов можно закончить, так как для начала самое главное просто интуитивно разобраться в этих вещах (если вас интересуют внутренности, то ссылки будут ниже).\n","\n"]},{"metadata":{"id":"tWjUtrTXS02w","colab_type":"text"},"cell_type":"markdown","source":["## Recurrent Neural Network (RNN)"]},{"metadata":{"id":"BFw3fBRNTFiT","colab_type":"text"},"cell_type":"markdown","source":["Наконец-то мы возвращаемся к нейронкам... Может вы знаете, а может и не совсем, но нейронки бывают не только с полносвязными слоями. Кроме полносвязной архитектуры существует множество других (сверточные, рекуррентные, генеративно-состязательные, со смешанными архитектурами и так далее)\n","\n","Поговорим сначала о рекуррентных сетях. Давайте допустим, что у вас есть 2 галстука: один с цветочком, а другой с оленем. И вы каждое утро встаете в универ и думаете, какой сегодня надеть галстук. Спустя какое-то время вам надоело думать и вы просто решили написать нейросеть, которая бы принимала решение за вас. Основываясь на знаниях с прошлого занятия вы решили написать обычную полносвязную нейросеть с 3 входами (время года, день недели, ваше настроение по шкале от 1 до 10) и двумя выходами (наденете галстук с цветочком или с оленем). Получилось что-то вроде такого:\n","![alt text](https://churchman.nl/wp-content/uploads/2015/07/network-1-e1437307622159.png)\n","\n","Далее, в течение некоторого времени вы собирали данные и наконец обучили вашу сеть. Но, как оказалось, она не работает (и это никак не связано с вашими умственными способностями). Просто вам стоило еще учитывать данные за день до этого (то есть, какой галстук вы надевали вчера). Чтобы решить эту задачу, вы принимаете решение доработать вашу нейронную сеть: вы добавляете в нее еще два входа. На первый новый вход вы подадите вчерашнее значение первого выхода вышей сети, а на второй новый вход подадите вчерашнее значение второго выхода сети. И таким образом, вы создадите рекуррентную нейронную сеть. \n","\n","Галстук мы вроде как смогли подобрать, но, что насчет чего-то более интересного. Обычно рекуррентные сети используют для предсказания временных рядов (например, какая будет цена на эфир завтра) или для генерации текста (например, чтобы ваш чатбот самостоятельно составлял предложения без вашей помощи) (https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist). И это конечно супер-круто, но простая rnn не справится с такой задачей на отлично, так как она никак не сможет вспомнить, какой галстук вы надевали позавчера, и это будет иметь для вас серьезные последствия.\n","\n","Рассмотрим пример более сложной задачи подробнее. Например, вы хотите, чтобы ваша нейросеть была четким пацаном и в любой непонятной ситуации могла подогнать вам четкую цитату, чтобы взбодрить корешей. Нейронка должна будет генерировать цитаты по очереди, одну за другой, чтобы вас не перестали уважать во дворе за повторы.\n","\n","В качестве примера рассмотрим некоторые цитаты с сайта http://citaty.ru/pacanskie/:\n","1. \"Не избегай драки, раны заживут быстрее, чем самооценка.\"\n","\n","2. \"Не беда, если нет друзей, беда если они фальшивые и продажные.\"\n","\n","3. \"Я не Минздрав — предупреждать не буду.\"\n","\n","4. \"Мне не важно прав он или нет, он мой брат, и я тебя за него разорву.\"\n","\n","Так вот. Вы берете составляете словарь из всех слов, которые здесь присутствуют (**точка это тоже слово**). Число входов сети будет равно числу слов в словаре, умноженному на два, а число выходов равно просто числу слов в словаре. Обучим ее на этих примерах. Смысл обучения будет таков. Вам нужно зациклить эти 4 цитаты, чтобы за одной шла другая, за последней первая. Получается, что мы всегда знаем, какое слово будет следующим и какое было предыдущим. Зная это, можно обучить нейросеть. Но после обучения вы заметите, что нейросеть путает первую и вторую цитаты (так как первое слово это \"не\", а предыдущее за ним \".\" => входные данные одинаковы), поэтому она постоянно выдает вам либо первую, либо вторую. Поэтому здесь уже надо думать... Решение для такой задачи лежит в таком виде рекуррентных нейронных, как LSTM.\n","\n"]},{"metadata":{"id":"90gPBYwqS7fI","colab_type":"text"},"cell_type":"markdown","source":["## Long Short-Term Memory (LSTM)"]},{"metadata":{"id":"VTIuFxvRTGTJ","colab_type":"text"},"cell_type":"markdown","source":["LSTM - это усовершенствованная rnn, которая может помнить дольше. Особо вдаваться в детали не буду (посмотрите видос внизу, там все круто рассказывается, если вам интересно), Скажу только, что в LSTM доабвляется обработка данных которые приходят с выходов на входы (то есть происходит учет более ранних событий). Именно благодаря этой обработке, rnn может запоминать дальше lstm. Ну и следовательно никаких проблем с пацанскими цитатами уже не будет.\n","\n","Ну а теперь перейдем к практике..."]},{"metadata":{"id":"o8-uyzlwTQ7Q","colab_type":"text"},"cell_type":"markdown","source":["# Практика"]},{"metadata":{"id":"_F0wau00nnEq","colab_type":"text"},"cell_type":"markdown","source":["На практике мы закрепим то, что прошли на теории. То есть поработаем с word2vec и lstm.\n","\n","Для начала установим gensim и скачаем готовую небольшую англоязычную модель word2vec, обученную гуглом на новостях\n","\n","**Прежде чем начать зайдите по ссылке: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ и добавьте файл к себе на диск(места не занимает)**\n","\n"]},{"metadata":{"id":"i6169KxcHXFV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":749},"outputId":"b5e38d1a-555b-42c7-a87e-0ade53132e98","executionInfo":{"status":"ok","timestamp":1541244519887,"user_tz":-180,"elapsed":12416,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["!pip install gensim"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting gensim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n","\u001b[K    100% |████████████████████████████████| 23.6MB 1.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n","Collecting smart-open>=1.2.1 (from gensim)\n","  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n","Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n","\u001b[K    100% |████████████████████████████████| 1.4MB 14.6MB/s \n","\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n","Collecting boto3 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/7e/fe8faa29e771a09c528ee70e1fd9b317006021c48311ecccc78c22ebe739/boto3-1.9.37-py2.py3-none-any.whl (128kB)\n","\u001b[K    100% |████████████████████████████████| 133kB 26.9MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n","Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 22.5MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n","Collecting botocore<1.13.0,>=1.12.37 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/35/1461771f778b67984a75a9853a2b3ed25e65a4345e669a81b50c67c930ab/botocore-1.12.37-py2.py3-none-any.whl (4.7MB)\n","\u001b[K    100% |████████████████████████████████| 4.7MB 7.6MB/s \n","\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.37->boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n","\u001b[K    100% |████████████████████████████████| 552kB 23.5MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.37->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n","Building wheels for collected packages: smart-open, bz2file\n","  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n","  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n","Successfully built smart-open bz2file\n","Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n","Successfully installed boto-2.49.0 boto3-1.9.37 botocore-1.12.37 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"],"name":"stdout"}]},{"metadata":{"id":"NXKW41x1Aczw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":538},"outputId":"5ac8941c-842d-405a-bcc0-baee69d34dc2","executionInfo":{"status":"ok","timestamp":1541246255036,"user_tz":-180,"elapsed":3166,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["from google.colab import drive # подключим возможность видеть наш гугл диск\n","drive.mount('/content/drive')\n","!ls drive/My\\ Drive/ # проверка"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"," asmlabs.zip\n"," Audream.rar\n","'Colab Notebooks'\n"," exlsv.rar\n"," formuls.txt\n"," GoogleNews-vectors-negative300.bin.gz\n"," IT_АРКТИКА_2016-1.pdf\n"," lab11.rar\n"," lab4.gdoc\n"," labs.tar.gz\n"," libnnet.tar.gz\n"," lll\n"," MTS.pdf\n"," mywiki.txt\n"," project1.rar\n","'PSEEWM160P146021 — копия.png'\n"," rl_trading\n"," site.rar\n"," tokenizedwiki.txt\n"," Untitled0.ipynb\n","'voprosi k ekzameny (1).docx'\n","'voprosi k ekzameny.docx.gdoc'\n"," Ассемблер.zip\n","'Копия Искусственные нейронные сети.ipynb'\n"," курсоваямоя.docx\n","'Новый точечный рисунок (2).bmp'\n","'План поездки.gsheet'\n"," рафтинг\n"," сети.docx\n"],"name":"stdout"}]},{"metadata":{"id":"Osc0M7cuJKQa","colab_type":"text"},"cell_type":"markdown","source":["Ну теперь начнем. Подгрузим нашу модель в оперативную память и проверим работоспособность"]},{"metadata":{"id":"T4Gvng2aA3yJ","colab_type":"code","colab":{}},"cell_type":"code","source":["import gensim\n","path = \"drive/My Drive/GoogleNews-vectors-negative300.bin.gz\" # путь к модели\n","\n","model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, unicode_errors=\"ignore\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HtFdoNd-pPpg","colab_type":"text"},"cell_type":"markdown","source":["Попробуем получить векторы нескольких слов"]},{"metadata":{"id":"PZt9jOfFCEWO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2622},"outputId":"860349e8-69ad-475a-bc47-fd076a4db9c0","executionInfo":{"status":"ok","timestamp":1541246418812,"user_tz":-180,"elapsed":615,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["print(model['hello'])\n","print(model['leather'])\n","print(model['bag'])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[-0.05419922  0.01708984 -0.00527954  0.33203125 -0.25       -0.01397705\n"," -0.15039062 -0.265625    0.01647949  0.3828125  -0.03295898 -0.09716797\n"," -0.16308594 -0.04443359  0.00946045  0.18457031  0.03637695  0.16601562\n","  0.36328125 -0.25585938  0.375       0.171875    0.21386719 -0.19921875\n","  0.13085938 -0.07275391 -0.02819824  0.11621094  0.15332031  0.09082031\n","  0.06787109 -0.0300293  -0.16894531 -0.20800781 -0.03710938 -0.22753906\n","  0.26367188  0.012146    0.18359375  0.31054688 -0.10791016 -0.19140625\n","  0.21582031  0.13183594 -0.03515625  0.18554688 -0.30859375  0.04785156\n"," -0.10986328  0.14355469 -0.43554688 -0.0378418   0.10839844  0.140625\n"," -0.10595703  0.26171875 -0.17089844  0.39453125  0.12597656 -0.27734375\n"," -0.28125     0.14746094 -0.20996094  0.02355957  0.18457031  0.00445557\n"," -0.27929688 -0.03637695 -0.29296875  0.19628906  0.20703125  0.2890625\n"," -0.20507812  0.06787109 -0.43164062 -0.10986328 -0.2578125  -0.02331543\n","  0.11328125  0.23144531 -0.04418945  0.10839844 -0.2890625  -0.09521484\n"," -0.10351562 -0.0324707   0.07763672 -0.13378906  0.22949219  0.06298828\n","  0.08349609  0.02929688 -0.11474609  0.00534058 -0.12988281  0.02514648\n","  0.08789062  0.24511719 -0.11474609 -0.296875   -0.59375    -0.29492188\n"," -0.13378906  0.27734375 -0.04174805  0.11621094  0.28320312  0.00241089\n","  0.13867188 -0.00683594 -0.30078125  0.16210938  0.01171875 -0.13867188\n","  0.48828125  0.02880859  0.02416992  0.04736328  0.05859375 -0.23828125\n","  0.02758789  0.05981445 -0.03857422  0.06933594  0.14941406 -0.10888672\n"," -0.07324219  0.08789062  0.27148438  0.06591797 -0.37890625 -0.26171875\n"," -0.13183594  0.09570312 -0.3125      0.10205078  0.03063965  0.23632812\n","  0.00582886  0.27734375  0.20507812 -0.17871094 -0.31445312 -0.01586914\n","  0.13964844  0.13574219  0.0390625  -0.29296875  0.234375   -0.33984375\n"," -0.11816406  0.10644531 -0.18457031 -0.02099609  0.02563477  0.25390625\n","  0.07275391  0.13574219 -0.00138092 -0.2578125  -0.2890625   0.10107422\n","  0.19238281 -0.04882812  0.27929688 -0.3359375  -0.07373047  0.01879883\n"," -0.10986328 -0.04614258  0.15722656  0.06689453 -0.03417969  0.16308594\n","  0.08642578  0.44726562  0.02026367 -0.01977539  0.07958984  0.17773438\n"," -0.04370117 -0.00952148  0.16503906  0.17285156  0.23144531 -0.04272461\n","  0.02355957  0.18359375 -0.41601562 -0.01745605  0.16796875  0.04736328\n","  0.14257812  0.08496094  0.33984375  0.1484375  -0.34375    -0.14160156\n"," -0.06835938 -0.14648438 -0.02844238  0.07421875 -0.07666016  0.12695312\n","  0.05859375 -0.07568359 -0.03344727  0.23632812 -0.16308594  0.16503906\n","  0.1484375  -0.2421875  -0.3515625  -0.30664062  0.00491333  0.17675781\n","  0.46289062  0.14257812 -0.25       -0.25976562  0.04370117  0.34960938\n","  0.05957031  0.07617188 -0.02868652 -0.09667969 -0.01281738  0.05859375\n"," -0.22949219 -0.1953125  -0.12207031  0.20117188 -0.42382812  0.06005859\n","  0.50390625  0.20898438  0.11230469 -0.06054688  0.33203125  0.07421875\n"," -0.05786133  0.11083984 -0.06494141  0.05639648  0.01757812  0.08398438\n","  0.13769531  0.2578125   0.16796875 -0.16894531  0.01794434  0.16015625\n","  0.26171875  0.31640625 -0.24804688  0.05371094 -0.0859375   0.17089844\n"," -0.39453125 -0.00156403 -0.07324219 -0.04614258 -0.16210938 -0.15722656\n","  0.21289062 -0.15820312  0.04394531  0.28515625  0.01196289 -0.26953125\n"," -0.04370117  0.37109375  0.04663086 -0.19726562  0.3046875  -0.36523438\n"," -0.23632812  0.08056641 -0.04248047 -0.14648438 -0.06225586 -0.0534668\n"," -0.05664062  0.18945312  0.37109375 -0.22070312  0.04638672  0.02612305\n"," -0.11474609  0.265625   -0.02453613  0.11083984 -0.02514648 -0.12060547\n","  0.05297852  0.07128906  0.00063705 -0.36523438 -0.13769531 -0.12890625]\n","[ 0.07080078  0.21679688 -0.22070312 -0.02783203  0.08691406  0.02331543\n"," -0.20605469 -0.2109375   0.05664062  0.36914062 -0.18847656 -0.3203125\n","  0.06591797 -0.15332031 -0.10498047 -0.07324219  0.05078125  0.18554688\n"," -0.04541016 -0.13183594 -0.234375    0.09619141  0.01916504  0.0222168\n","  0.05834961  0.19335938 -0.15527344  0.27734375  0.01177979 -0.18847656\n","  0.00927734  0.08789062  0.03979492  0.453125   -0.16210938 -0.10351562\n","  0.04467773  0.06152344 -0.18554688 -0.06054688  0.00314331  0.10302734\n"," -0.16894531 -0.06640625 -0.10986328 -0.234375    0.20703125  0.25\n","  0.12353516 -0.02355957  0.00952148  0.1953125   0.00524902  0.06396484\n"," -0.03149414 -0.14355469 -0.00052261  0.171875   -0.10888672  0.19335938\n"," -0.125      -0.28125    -0.13769531 -0.33398438 -0.19433594  0.19140625\n"," -0.21289062  0.03051758  0.3125      0.3046875   0.44335938  0.17480469\n","  0.07421875  0.078125   -0.27929688  0.00292969  0.14941406 -0.00994873\n"," -0.17871094  0.05932617  0.20410156 -0.42382812 -0.13378906 -0.16894531\n"," -0.20605469 -0.17773438  0.05371094  0.2734375  -0.12890625  0.06640625\n"," -0.3828125   0.21582031 -0.0390625   0.15722656 -0.04003906 -0.2421875\n","  0.17089844 -0.13769531  0.21777344 -0.4921875  -0.19335938 -0.20507812\n"," -0.13574219 -0.08789062 -0.05908203 -0.0390625   0.49023438  0.04394531\n","  0.03979492  0.05957031 -0.02905273  0.05859375 -0.01843262  0.26367188\n"," -0.03979492 -0.06689453 -0.38671875  0.13574219  0.00210571  0.16308594\n","  0.11816406  0.07324219 -0.33007812 -0.17773438  0.07714844 -0.16894531\n"," -0.22070312 -0.09667969  0.21386719  0.01403809 -0.11816406  0.08935547\n"," -0.28125     0.0859375   0.00270081  0.36132812 -0.16308594  0.36328125\n"," -0.19921875 -0.01672363  0.09423828 -0.0267334  -0.07470703 -0.19726562\n","  0.0009613   0.07568359  0.10839844  0.25195312  0.00680542  0.15234375\n","  0.13085938  0.05664062 -0.26757812  0.21972656  0.10302734 -0.12597656\n"," -0.4765625   0.16894531 -0.24511719  0.07324219  0.03442383 -0.21582031\n","  0.04858398 -0.04345703 -0.22558594  0.27539062 -0.0703125  -0.08740234\n"," -0.17773438 -0.02746582 -0.24121094  0.03930664 -0.14648438 -0.12353516\n","  0.04638672 -0.02441406  0.19238281 -0.42578125 -0.2109375  -0.0213623\n"," -0.30859375 -0.109375   -0.03369141 -0.03393555 -0.03808594 -0.05712891\n","  0.17382812 -0.25976562  0.16894531 -0.12695312  0.125       0.03112793\n"," -0.20410156  0.21289062  0.1953125   0.19921875  0.13183594 -0.31054688\n"," -0.04858398 -0.18066406 -0.03417969 -0.28125    -0.04150391 -0.03173828\n","  0.10742188 -0.0177002  -0.12597656 -0.26757812 -0.14355469  0.00469971\n"," -0.10644531  0.12597656 -0.09082031 -0.09326172  0.31054688  0.14160156\n"," -0.08935547  0.17578125 -0.09472656  0.09375    -0.33398438 -0.06591797\n"," -0.34570312  0.38671875  0.13671875  0.04492188  0.09960938  0.08398438\n","  0.15429688 -0.0291748   0.26953125  0.16015625  0.00964355  0.15039062\n"," -0.16601562  0.20800781 -0.22363281  0.0291748  -0.00787354 -0.23828125\n","  0.41015625  0.01477051  0.14648438 -0.10449219 -0.13085938 -0.09570312\n","  0.11962891 -0.12597656  0.07128906 -0.14160156  0.19042969  0.2890625\n","  0.30273438  0.09667969 -0.09423828 -0.3359375  -0.06030273 -0.17089844\n","  0.21191406 -0.10058594  0.203125    0.12207031  0.06542969  0.06835938\n"," -0.078125   -0.03295898 -0.00357056 -0.16015625  0.02172852  0.07226562\n","  0.00234985  0.21679688 -0.04077148  0.06005859  0.11474609 -0.0703125\n","  0.09228516 -0.26953125 -0.36523438  0.42578125  0.01306152 -0.05004883\n","  0.21582031 -0.10546875 -0.07275391  0.00166321 -0.2890625  -0.06054688\n"," -0.03491211  0.06494141 -0.08789062 -0.24121094  0.02905273  0.27734375\n","  0.39257812  0.00622559  0.20214844  0.04785156  0.00402832  0.16699219]\n","[-0.03515625  0.15234375 -0.12402344  0.13378906 -0.11328125 -0.0133667\n"," -0.16113281  0.14648438 -0.06835938  0.140625   -0.06005859 -0.3046875\n","  0.20996094 -0.04345703 -0.2109375  -0.05957031 -0.05053711  0.10253906\n","  0.19042969 -0.09423828  0.18847656 -0.07958984 -0.11035156 -0.07910156\n","  0.06347656 -0.15527344 -0.18945312  0.11132812  0.27539062 -0.06787109\n","  0.01806641  0.06689453  0.2578125   0.0324707  -0.24609375 -0.05541992\n","  0.01013184  0.24121094 -0.21875     0.07568359 -0.09814453 -0.16113281\n","  0.16503906 -0.09521484 -0.16601562 -0.41796875  0.0300293   0.19433594\n","  0.2890625   0.12695312 -0.19824219 -0.05517578  0.04296875 -0.10107422\n","  0.07324219 -0.13378906  0.265625   -0.00466919  0.19628906 -0.10839844\n","  0.14941406  0.1484375   0.09619141  0.21777344 -0.08544922 -0.02819824\n","  0.02539062 -0.03759766  0.23242188  0.19628906  0.27539062  0.09130859\n","  0.23730469  0.09033203 -0.28515625  0.05932617  0.06591797 -0.01794434\n"," -0.00055313 -0.1796875   0.05615234 -0.12207031 -0.09863281 -0.05786133\n"," -0.09375    -0.30273438 -0.06396484 -0.00744629 -0.17871094  0.08544922\n"," -0.20410156  0.33789062  0.00228882 -0.39453125 -0.14453125 -0.328125\n"," -0.12695312 -0.08544922  0.15234375  0.03662109 -0.1484375   0.05566406\n","  0.02844238  0.07519531 -0.21484375 -0.15722656  0.3359375  -0.04736328\n"," -0.00405884 -0.19726562  0.27929688  0.05566406 -0.10058594 -0.00811768\n"," -0.20703125  0.03295898 -0.14550781 -0.15917969  0.16503906  0.234375\n","  0.03588867  0.04296875 -0.25        0.1171875  -0.07714844  0.00521851\n","  0.125       0.08886719  0.15527344 -0.02185059 -0.15234375 -0.12890625\n"," -0.34765625 -0.13769531 -0.18164062  0.37695312  0.14160156 -0.03051758\n","  0.08203125  0.09423828 -0.23242188 -0.28320312 -0.21191406 -0.09130859\n","  0.13183594 -0.08886719 -0.26367188 -0.13378906  0.15429688 -0.09619141\n","  0.16992188 -0.02124023 -0.02148438  0.17871094  0.17578125 -0.04980469\n"," -0.34375    -0.07568359 -0.02380371  0.26171875 -0.26171875 -0.18945312\n","  0.46289062 -0.17285156  0.18847656 -0.09130859  0.28125     0.11621094\n","  0.05981445 -0.171875   -0.15820312  0.19042969  0.03759766  0.20898438\n"," -0.36523438 -0.03198242  0.28710938 -0.33007812 -0.01953125  0.14941406\n"," -0.08447266 -0.02612305  0.13476562  0.16992188 -0.06347656 -0.06982422\n","  0.00747681  0.21679688 -0.05639648  0.41210938  0.06347656  0.03039551\n"," -0.08984375 -0.03637695 -0.17285156  0.20019531 -0.0067749  -0.15527344\n"," -0.2421875  -0.11230469 -0.25390625  0.01330566 -0.40039062 -0.02038574\n"," -0.14160156  0.05395508 -0.10595703  0.21386719  0.23535156  0.09277344\n","  0.33203125  0.12109375 -0.35351562  0.25390625 -0.04614258  0.22558594\n","  0.00485229  0.14257812 -0.20410156 -0.12402344 -0.02001953  0.03417969\n","  0.03833008  0.0177002  -0.05322266  0.06591797  0.02783203 -0.12451172\n"," -0.125      -0.28125     0.17773438  0.18066406 -0.03417969 -0.17089844\n","  0.20214844  0.10351562 -0.03295898  0.10839844  0.31445312 -0.03112793\n","  0.09912109 -0.08740234  0.23144531 -0.30273438  0.10839844  0.05566406\n"," -0.22949219 -0.29296875  0.08642578 -0.05859375 -0.05810547 -0.11230469\n","  0.19238281  0.12304688  0.09472656 -0.07568359 -0.17089844 -0.00131226\n"," -0.07080078 -0.20605469  0.12695312  0.26757812 -0.45507812 -0.41796875\n"," -0.11132812  0.08300781 -0.09521484 -0.04980469 -0.5234375   0.10498047\n"," -0.16796875 -0.2890625  -0.07861328  0.3203125   0.12207031  0.09863281\n","  0.02099609  0.03015137 -0.01165771  0.00177002  0.22558594 -0.01379395\n","  0.22851562 -0.00964355  0.09912109  0.08105469 -0.10839844  0.1484375\n"," -0.16601562 -0.14453125 -0.00750732 -0.25195312  0.17285156  0.01177979\n"," -0.02062988 -0.40820312 -0.01538086 -0.04956055 -0.1640625  -0.03222656]\n"],"name":"stdout"}]},{"metadata":{"id":"BlNevqtwpV8U","colab_type":"text"},"cell_type":"markdown","source":["Проверим размерность одного вектора"]},{"metadata":{"id":"Lj2sjBK-Cm6f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"fbe77355-9122-4fa9-f531-b46be6fdc017","executionInfo":{"status":"ok","timestamp":1541246425081,"user_tz":-180,"elapsed":590,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["print(len(model['hello']))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["300\n"],"name":"stdout"}]},{"metadata":{"id":"0NP4rsLjDntq","colab_type":"text"},"cell_type":"markdown","source":["Решим классическую задачу по анализу тональности отзывов imdb. Обратите внимание, что модель word2vec англоязычная и датасет тоже англоязычный.\n","\n","Подгрузим датасет\n","\n","Необходимо также знать о том, что в керасе по умолчанию в датасете imdb слова закодированы следующим образом: у каждого слова есть свой индекс (чем меньше индекс, тем чаще слово встречается в датасете). Переменная INDEX_FROM обозначает, с какого индекса мы начинаем отсчет (3, потому что в керасе по умолчанию индексы 0, 1 и 2 определены для спецсимволов, а именно 0 - <PAD\\>, 1 - <START\\>, 2 - <UNK\\>).\n","    \n","А еще мы возьмем только часть датасета, потому что в гугл колабе не хватает оперативной памяти, чтобы обрабатывать весь"]},{"metadata":{"id":"_FUEfOYRD-qo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4de485e0-ebbe-4962-8c3f-8e1c2159b625","executionInfo":{"status":"ok","timestamp":1541246436482,"user_tz":-180,"elapsed":8401,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["from keras.datasets import imdb\n","\n","INDEX_FROM=3   # word index offset\n","\n","train,test = imdb.load_data(index_from=INDEX_FROM) # грузим\n","train_x,train_y = train\n","test_x,test_y = test\n","\n","test_x = train_x[train_x.shape[0] // 2:] # делим датасет\n","train_x = train_x[:train_x.shape[0] // 2]\n","\n","test_y = train_y[train_y.shape[0] // 2:]\n","train_y = train_y[:train_y.shape[0] // 2]\n","\n","train = [] # отчаянная попытка освободить память, чтобы ничего в конце не свалилось)\n","test = [] # если че, то это только для тех, кто запускает скрипт в ipython notebook (.ipnb)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"3-T7QAcMFstU","colab_type":"text"},"cell_type":"markdown","source":[" Ограничим длину каждого отзыва до 50 слов (иногда берется 80)"]},{"metadata":{"id":"gQGfx-8rFqI0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"94502434-ee92-413e-885d-61db513f73da","executionInfo":{"status":"ok","timestamp":1541246441849,"user_tz":-180,"elapsed":925,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["maxlen = 50\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","train_x = pad_sequences(train_x, maxlen=maxlen)\n","test_x = pad_sequences(test_x, maxlen=maxlen)\n","print('train_x shape:', train_x.shape)\n","print('test_x shape:', test_x.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["train_x shape: (12500, 50)\n","test_x shape: (12500, 50)\n"],"name":"stdout"}]},{"metadata":{"id":"BrZiO4Bl2C7E","colab_type":"text"},"cell_type":"markdown","source":["Далее надо будет закодированные слова превратить в нормальные и перевести сразу в word2vec:"]},{"metadata":{"id":"LRhiBvExZqIy","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","def to_w2v(word_list): # эта функция для более удобного перевода предложения в word2vec\n","    global model\n","    if len(word_list) > 50: # почему-то при переводе в нормальные слова число слов в последовательностях поменялось, поэтому тут либо убираем лишние, либо добавляем символ <UNK>\n","        word_list = word_list[0:50]\n","    elif len(word_list) < 50:\n","        word_list.append(\"<UNK>\")\n","    res = []\n","    for one in word_list:\n","        if one in model: # проверка на наличие данного слова в словаре\n","            res.append(np.asarray(model[one]))\n","        else:\n","            res.append(np.asarray([0.0 for _ in range(300)])) # если слова в словаре нет, то просто записываем нулевой вектор\n","            \n","    return np.asarray(res)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GgB6e6122LnJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n","word_to_id = imdb.get_word_index() # скачиваем индексы слов\n","word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()} # составялем словарь\n","word_to_id[\"<PAD>\"] = 0\n","word_to_id[\"<START>\"] = 1\n","word_to_id[\"<UNK>\"] = 2\n","\n","id_to_word = {value:key for key,value in word_to_id.items()} # переворачиваем его, чтобы ключом стал индекс, а не слово\n","\n","x_train = []\n","for one in train_x:\n","    s = ' '.join(id_to_word[id] for id in one) # составляем строку\n","    s = to_w2v(s.split()) # сразу переводим ее в word2vec-векторы\n","    x_train.append(s) # добавляем в список\n","\n","x_test = []\n","for one in test_x:\n","    s = ' '.join(id_to_word[id] for id in one)\n","    s = to_w2v(s.split())\n","    x_test.append(s)\n","    \n","train_x = [] # очередная попытка освободить память\n","test_x = []"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nIy4F2HtTeIn","colab_type":"text"},"cell_type":"markdown","source":["Приведем список к numpy-array"]},{"metadata":{"id":"aN0CK4avdaDW","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train = np.asarray(x_train)\n","x_test = np.asarray(x_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZvYv9Wkjeghl","colab_type":"text"},"cell_type":"markdown","source":["Отлично! Теперь создадим нашу первую lstm-сеть "]},{"metadata":{"id":"RHVtXumIIS_e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"69749eb5-c02e-4dd5-f305-68aae21d2ee9","executionInfo":{"status":"ok","timestamp":1541246880862,"user_tz":-180,"elapsed":907,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","\n","net = Sequential()\n","\n","net.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5, input_shape=(maxlen, 300,)))# (max_length, the embedding_dimensions, )\n","net.add(Dense(1, activation='sigmoid'))\n","\n","# try using different optimizers and different optimizer configs\n","net.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","net.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_2 (LSTM)                (None, 128)               219648    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 129       \n","=================================================================\n","Total params: 219,777\n","Trainable params: 219,777\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"ixVkqln6WgaJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"565b4f9d-ac41-40b2-d988-d4ffd8acf004","executionInfo":{"status":"ok","timestamp":1541246886166,"user_tz":-180,"elapsed":606,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["x_test.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12500, 50, 300)"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"GcfUDV45eDt2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b9effdb4-2fd2-4998-99cb-66961a0d9ee6","executionInfo":{"status":"ok","timestamp":1541246887491,"user_tz":-180,"elapsed":603,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["test_y[43]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"1pSUG6TCdCUV","colab_type":"text"},"cell_type":"markdown","source":["А теперь обучим нашу LSTM и сразу же проверим результаты обучения на тестовой выборке"]},{"metadata":{"id":"_lILGfgEdJzU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1128},"outputId":"3f32e342-eea4-4913-ee42-78512505bc74","executionInfo":{"status":"ok","timestamp":1541248161304,"user_tz":-180,"elapsed":1271589,"user":{"displayName":"Лопата Комар","photoUrl":"","userId":"15181977420584180779"}}},"cell_type":"code","source":["net.fit(x_train, train_y,\n","          batch_size=50,\n","          epochs=30,\n","          validation_data=(x_test, test_y))\n","score, acc = net.evaluate(x_test, test_y,\n","                            batch_size=50)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Train on 12500 samples, validate on 12500 samples\n","Epoch 1/30\n","12500/12500 [==============================] - 45s 4ms/step - loss: 0.6208 - acc: 0.6521 - val_loss: 0.5269 - val_acc: 0.7378\n","Epoch 2/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.5777 - acc: 0.6955 - val_loss: 0.4915 - val_acc: 0.7676\n","Epoch 3/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.5524 - acc: 0.7182 - val_loss: 0.4735 - val_acc: 0.7680\n","Epoch 4/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.5417 - acc: 0.7266 - val_loss: 0.4939 - val_acc: 0.7652\n","Epoch 5/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.5144 - acc: 0.7482 - val_loss: 0.4493 - val_acc: 0.7869\n","Epoch 6/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4865 - acc: 0.7639 - val_loss: 0.4399 - val_acc: 0.7929\n","Epoch 7/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4747 - acc: 0.7687 - val_loss: 0.4338 - val_acc: 0.7963\n","Epoch 8/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4639 - acc: 0.7753 - val_loss: 0.4283 - val_acc: 0.8005\n","Epoch 9/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4458 - acc: 0.7888 - val_loss: 0.4243 - val_acc: 0.8005\n","Epoch 10/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4369 - acc: 0.7960 - val_loss: 0.4192 - val_acc: 0.8062\n","Epoch 11/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4261 - acc: 0.7994 - val_loss: 0.4102 - val_acc: 0.8087\n","Epoch 12/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4180 - acc: 0.8027 - val_loss: 0.4034 - val_acc: 0.8122\n","Epoch 13/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.4063 - acc: 0.8152 - val_loss: 0.4009 - val_acc: 0.8142\n","Epoch 14/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3973 - acc: 0.8163 - val_loss: 0.3967 - val_acc: 0.8182\n","Epoch 15/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3958 - acc: 0.8172 - val_loss: 0.3984 - val_acc: 0.8202\n","Epoch 16/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3892 - acc: 0.8190 - val_loss: 0.3911 - val_acc: 0.8202\n","Epoch 17/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3742 - acc: 0.8284 - val_loss: 0.3893 - val_acc: 0.8215\n","Epoch 18/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3708 - acc: 0.8293 - val_loss: 0.3876 - val_acc: 0.8217\n","Epoch 19/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3606 - acc: 0.8343 - val_loss: 0.3860 - val_acc: 0.8218\n","Epoch 20/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3586 - acc: 0.8380 - val_loss: 0.3900 - val_acc: 0.8203\n","Epoch 21/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3530 - acc: 0.8383 - val_loss: 0.3863 - val_acc: 0.8246\n","Epoch 22/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3445 - acc: 0.8465 - val_loss: 0.3896 - val_acc: 0.8189\n","Epoch 23/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3354 - acc: 0.8476 - val_loss: 0.3879 - val_acc: 0.8228\n","Epoch 24/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3346 - acc: 0.8512 - val_loss: 0.3893 - val_acc: 0.8201\n","Epoch 25/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3269 - acc: 0.8551 - val_loss: 0.3925 - val_acc: 0.8234\n","Epoch 26/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3174 - acc: 0.8579 - val_loss: 0.3882 - val_acc: 0.8246\n","Epoch 27/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3075 - acc: 0.8634 - val_loss: 0.3976 - val_acc: 0.8241\n","Epoch 28/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.3029 - acc: 0.8676 - val_loss: 0.3909 - val_acc: 0.8230\n","Epoch 29/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.2953 - acc: 0.8690 - val_loss: 0.3963 - val_acc: 0.8238\n","Epoch 30/30\n","12500/12500 [==============================] - 42s 3ms/step - loss: 0.2936 - acc: 0.8684 - val_loss: 0.3969 - val_acc: 0.8260\n","12500/12500 [==============================] - 10s 808us/step\n","Test score: 0.39689023679494856\n","Test accuracy: 0.8259999945163726\n"],"name":"stdout"}]},{"metadata":{"id":"bgGyyDG_m1RT","colab_type":"text"},"cell_type":"markdown","source":["Точность 82,5%.. Это не так уж и плохо. Вы можете поэкспериментировать с параметрами, подобавлять еще слоев (я просто очень сильно старался влезть в ограничение по оперативной памяти). Но самое главное, чему мы сегодня научились - это использование word2vec и LSTM. И все"]},{"metadata":{"id":"nchIFVHtnbyy","colab_type":"text"},"cell_type":"markdown","source":["На следующем занятии я бы хотел поговорить немного про картинки, сверточные сети, object detection и все такое)"]},{"metadata":{"id":"fRCq-jYBoJBJ","colab_type":"text"},"cell_type":"markdown","source":["# Домашнее задание"]},{"metadata":{"id":"mxEsP3LuoMGz","colab_type":"text"},"cell_type":"markdown","source":["Вам необходимо улучшить то, что вы сделали в прошлом задании... Нужно заменить перевод слов в векторы на word2vec, используя gensim (https://nlpub.ru/Russian_Distributional_Thesaurus - здесь можно найти модель на 500мб и спокойно с ней работать). А также переработать вашу нейронку, а именно: перейти с dense-слоев на lstm (подсказка: конечный слой все равно лучше делать dense). \n","\n","После этого нужно подключиться к твиттеру через api (как - https://www.youtube.com/watch?v=o_OZdbCzHUA). \n","\n","Ну и в конце концов сделать так, чтобы вы вводили слово в вашего бота (например, \"Путин\"), а дальше бы по этому слову искались все твиты. Каждый твит бы обрабатывался вашей нейронкой. В итоге программа бы посчитала среднее по всем твитам и выдавала бы ответ (типа \"Популярность \"Путин\" - N%\")"]},{"metadata":{"id":"ZPSyuKjeLU1h","colab_type":"text"},"cell_type":"markdown","source":["На этом все) Подписывайтесь на канал, ставьте лайки, нажимайте колокольчик"]},{"metadata":{"id":"A6z8IE8vLUrX","colab_type":"text"},"cell_type":"markdown","source":["# Полезные ссылки"]},{"metadata":{"id":"VrzUhnZ1LZEq","colab_type":"text"},"cell_type":"markdown","source":["https://machinelearningmastery.com/gentle-introduction-bag-words-model/ (подробнее о bag-of-words)\n","\n","https://nlpub.ru/Russian_Distributional_Thesaurus (здесь можно почитать о word2vec и вообще о nlp, а еще можно скачать готовые модели word2vec)\n","\n","http://nlpx.net/archives/179 (про word2vec)\n","\n","https://youtu.be/EqWm8A-dRYg (тут предсказывают цену на биткоин с помощью анализа тональности постов на реддите)\n","\n","https://youtu.be/WCUNPb-5EYI (хорошее объяснение rnn и lstm)\n","\n","https://rare-technologies.com/word2vec-tutorial/ (туториал по генсиму и ворд2кеку)\n","\n","https://habr.com/post/249215/ (куча примеров по word2vec, поможет понять принцип устройства)\n","\n","Решение этой же задачи но без word2vec: \n"," + https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py \n"," + https://www.asozykin.ru/courses/nnpython-lab3"]},{"metadata":{"id":"M1PAEHY1LYbL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}